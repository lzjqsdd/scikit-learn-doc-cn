
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
  
    <title>4.2. 特征提取 &#8212; scikit-learn 0.18.1 documentation</title>
  <!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" media="screen" />
  <link rel="stylesheet" href="../_static/css/bootstrap-responsive.css"/>

    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.18.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/js/copybutton.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="top" title="scikit-learn 0.18.1 documentation" href="../index.html" />
    <link rel="up" title="4. Dataset transformations" href="../data_transforms.html" />
    <link rel="next" title="4.3. 数据预处理" href="preprocessing.html" />
    <link rel="prev" title="4.1. Pipeline and FeatureUnion: combining estimators" href="pipeline.html" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="../_static/js/bootstrap.min.js" type="text/javascript"></script>
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/feature_extraction.html" />

  <script type="text/javascript">
    $("div.buttonNext, div.buttonPrevious").hover(
       function () {
           $(this).css('background-color', '#FF9C34');
       },
       function () {
           $(this).css('background-color', '#A7D6E2');
       }
    );
  </script>

  </head>
  <body role="document">

<div class="header-wrapper">
    <div class="header">
        <p class="logo"><a href="../index.html">
            <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
        </a>
        </p><div class="navbar">
            <ul>
                <li><a href="../index.html">主页</a></li>
                <li><a href="../install.html">安装</a></li>
                <li class="btn-li"><div class="btn-group">
              <a href="../documentation.html">文档</a>
              <a class="btn dropdown-toggle" data-toggle="dropdown">
                 <span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
            <li class="link-title">Scikit-learn 0.17 (stable)</li>
            <li><a href="../tutorial/index.html">入门指南</a></li>
            <li><a href="../user_guide.html">使用手册</a></li>
            <li><a href="classes.html">API</a></li>
            <li><a href="../faq.html">FAQ</a></li>
            <li><a href="../developers.html">贡献</a></li>
            <li class="divider"></li>
                <li><a href="http://scikit-learn.org/dev/documentation.html">Scikit-learn 0.18 (development)</a></li>
                <li><a href="http://scikit-learn.org/0.16/documentation.html">Scikit-learn 0.16</a></li>
				<li><a href="../_downloads/user_guide.pdf">PDF 文档</a></li>
              </ul>
            </div>
        </li>
            <li><a href="../auto_examples/index.html">例子</a></li>
            </ul>

            <div class="search_form">
                <div id="cse" style="width: 100%;"></div>
            </div>
        </div> <!-- end navbar --></div>
</div>


<!-- Github "fork me" ribbon -->
<a href="https://github.com/lzjqsdd/scikit-learn-doc-cn">
  <img class="fork-me"
       style="position: absolute; top: 0; right: 0; border: 0;"
       src="../_static/img/forkme.png"
       alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
    <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
        <div class="rel">
    

  <!-- rellinks[1:] is an ugly hack to avoid link to module
  index -->
        <div class="rellink">
        <a href="pipeline.html"
        accesskey="P">Previous
        <br/>
        <span class="smallrellink">
        4.1. Pipeline...
        </span>
            <span class="hiddenrellink">
            4.1. Pipeline and FeatureUnion: combining estimators
            </span>
        </a>
        </div>

    <!-- Ad a link to the 'up' page -->
        <div class="spacer">
        &nbsp;
        </div>
        <div class="rellink">
        <a href="../data_transforms.html">
        Up
        <br/>
        <span class="smallrellink">
        4. Dataset tr...
        </span>
            <span class="hiddenrellink">
            4. Dataset transformations
            </span>
            
        </a>
        </div>
    </div>
    
      <p class="doc-version">This documentation is for scikit-learn <strong>version 0.18.1</strong> &mdash; <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
    <p class="citing">If you use the software, please consider <a href="../about.html#citing-scikit-learn">citing scikit-learn</a>.</p>
    <ul>
<li><a class="reference internal" href="#">4.2. 特征提取</a><ul>
<li><a class="reference internal" href="#dict-feature-extraction">4.2.1. 加载字典的中的特征</a></li>
<li><a class="reference internal" href="#feature-hashing">4.2.2. 特征哈希</a><ul>
<li><a class="reference internal" href="#id5">4.2.2.1. 实现细节</a></li>
</ul>
</li>
<li><a class="reference internal" href="#text-feature-extraction">4.2.3. 文本特征提取</a><ul>
<li><a class="reference internal" href="#id7">4.2.3.1. 体现：词袋模型</a></li>
<li><a class="reference internal" href="#id8">4.2.3.2. 稀疏</a></li>
<li><a class="reference internal" href="#common-vectorizer-usage">4.2.3.3. 通常向量化使用Common Vectorizer usage</a></li>
<li><a class="reference internal" href="#tfidf">4.2.3.4. Tf–idf算法 字词权值</a></li>
<li><a class="reference internal" href="#id12">4.2.3.5. 文档编码 转码</a></li>
<li><a class="reference internal" href="#id13">4.2.3.6. 应用与例子</a></li>
<li><a class="reference internal" href="#id14">4.2.3.7. 词袋子模型表示法的限制</a></li>
<li><a class="reference internal" href="#vectorizing-a-large-text-corpus-with-the-hashing-trick">4.2.3.8. Vectorizing a large text corpus with the hashing trick</a></li>
<li><a class="reference internal" href="#performing-out-of-core-scaling-with-hashingvectorizer">4.2.3.9. Performing out-of-core scaling with HashingVectorizer</a></li>
<li><a class="reference internal" href="#customizing-the-vectorizer-classes">4.2.3.10. Customizing the vectorizer classes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#image-feature-extraction">4.2.4. Image feature extraction</a><ul>
<li><a class="reference internal" href="#patch-extraction">4.2.4.1. Patch extraction</a></li>
<li><a class="reference internal" href="#connectivity-graph-of-an-image">4.2.4.2. Connectivity graph of an image</a></li>
</ul>
</li>
</ul>
</li>
</ul>

    </div>
</div>

<input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
<label for="nav-trigger"></label>




      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="feature-extraction">
<span id="id1"></span><h1>4.2. 特征提取<a class="headerlink" href="#feature-extraction" title="Permalink to this headline">¶</a></h1>
<p>模块 <a class="reference internal" href="classes.html#module-sklearn.feature_extraction" title="sklearn.feature_extraction"><code class="xref py py-mod docutils literal"><span class="pre">sklearn.feature_extraction</span></code></a> 可以用来提取多种格式的数据集中，符合机器学习算法中支持的特征，如文本和图像</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">特征提取与 <a class="reference internal" href="feature_selection.html#feature-selection"><span class="std std-ref">特征选择(Feature selection)</span></a> 特征选择有很大的不同:
模型意义在于把复杂的数据，如文本和图像，转化为数字特征，从而在机器学习中使用。后者是一个机器学习中应用这些特征的方法</p>
</div>
<div class="section" id="dict-feature-extraction">
<span id="id2"></span><h2>4.2.1. 加载字典的中的特征<a class="headerlink" href="#dict-feature-extraction" title="Permalink to this headline">¶</a></h2>
<p>类 <a class="reference internal" href="generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-class docutils literal"><span class="pre">DictVectorizer</span></code></a> 可以把特征向量转化成标准的Python字典对象的一个列表，
同时也是被scikit-learn的估计器使用的一个NumPy/SciPy体现(ndarray)</p>
<p>即使处理时并不是特别快，python的字典有易于使用的优势，适用于稀疏情景(缺失特征不会被存储)，存储特征的名字和值。</p>
<p>类 <a class="reference internal" href="generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-class docutils literal"><span class="pre">DictVectorizer</span></code></a> 实现了所谓 one-of-K 或 &#8220;one-hot&#8221; 的方法来使用范畴(即离散的)特征 。
范畴特征是一个键值对，其值被约束为离散的无序列表</p>
<blockquote>
<div>(如话题标志，对象类型，标签，名字等)。</div></blockquote>
<p>在下面例子中 &#8220;city&#8221; 是一个绝对变量而  disc是一个 &#8220;temperature&#8221; 传统的数值特征</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">measurements</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s1">&#39;city&#39;</span><span class="p">:</span> <span class="s1">&#39;Dubai&#39;</span><span class="p">,</span> <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="mf">33.</span><span class="p">},</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s1">&#39;city&#39;</span><span class="p">:</span> <span class="s1">&#39;London&#39;</span><span class="p">,</span> <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="mf">12.</span><span class="p">},</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s1">&#39;city&#39;</span><span class="p">:</span> <span class="s1">&#39;San Fransisco&#39;</span><span class="p">,</span> <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="mf">18.</span><span class="p">},</span>
<span class="gp">... </span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="k">import</span> <span class="n">DictVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">measurements</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[  1.,   0.,   0.,  33.],</span>
<span class="go">       [  0.,   1.,   0.,  12.],</span>
<span class="go">       [  0.,   0.,   1.,  18.]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
<span class="go">[&#39;city=Dubai&#39;, &#39;city=London&#39;, &#39;city=San Fransisco&#39;, &#39;temperature&#39;]</span>
</pre></div>
</div>
<p>类 <a class="reference internal" href="generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-class docutils literal"><span class="pre">DictVectorizer</span></code></a> 也是一个有用的转化形式，主要应用于自然语言处理(NLP)中分类器的训练模型，典型应用于在兴趣文本中提取特征序列.</p>
<p>比如说，我们有一个算法来提取词性标签作为补充标签，来训练序列分类器(如chunker概括大意)。
下面的字典展示了一个小例子，提取在例句 &#8216;The cat sat on the mat.&#8217; 中sat周围的特征:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pos_window</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s1">&#39;word-2&#39;</span><span class="p">:</span> <span class="s1">&#39;the&#39;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s1">&#39;pos-2&#39;</span><span class="p">:</span> <span class="s1">&#39;DT&#39;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s1">&#39;word-1&#39;</span><span class="p">:</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s1">&#39;pos-1&#39;</span><span class="p">:</span> <span class="s1">&#39;NN&#39;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s1">&#39;word+1&#39;</span><span class="p">:</span> <span class="s1">&#39;on&#39;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s1">&#39;pos+1&#39;</span><span class="p">:</span> <span class="s1">&#39;PP&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="p">},</span>
<span class="gp">... </span>    <span class="c1"># in a real application one would extract many such dictionaries</span>
<span class="gp">... </span><span class="p">]</span>
</pre></div>
</div>
<p>以上形式可以被向量化成一个稀疏二维矩阵，从而作为参数传递给分类器(或经过:class:<cite>text.TfidfTransformer</cite> 的加工标准化):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pos_vectorized</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">pos_window</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pos_vectorized</span>                
<span class="go">&lt;1x6 sparse matrix of type &#39;&lt;... &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">    with 6 stored elements in Compressed Sparse ... format&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pos_vectorized</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[ 1.,  1.,  1.,  1.,  1.,  1.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
<span class="go">[&#39;pos+1=PP&#39;, &#39;pos-1=NN&#39;, &#39;pos-2=DT&#39;, &#39;word+1=on&#39;, &#39;word-1=cat&#39;, &#39;word-2=the&#39;]</span>
</pre></div>
</div>
<p>正如你所想的，如果在文档全集中进行提取，结果矩阵将会非常巨大(大量one-hot-features)，
他们中的大部分通常将会是0。所以为了使这个矩阵的稀疏数据结构存储在内存中，类 <code class="docutils literal"><span class="pre">DictVectorizer</span></code> 默认使用了一个 <code class="docutils literal"><span class="pre">scipy.sparse</span></code> 矩阵
而不是 <code class="docutils literal"><span class="pre">numpy.ndarray</span></code>。</p>
</div>
<div class="section" id="feature-hashing">
<span id="id3"></span><h2>4.2.2. 特征哈希<a class="headerlink" href="#feature-hashing" title="Permalink to this headline">¶</a></h2>
<p>类 <a class="reference internal" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-class docutils literal"><span class="pre">FeatureHasher</span></code></a> 是一个快速且低内存消耗的向量化方法，使用了 <a class="reference external" href="https://en.wikipedia.org/wiki/Feature_hashing">feature hashing</a> 技术，或可称为&#8221;hashing trick&#8221;。
没有像矢量化那样，为计算训练得到的特征建立哈西表，类 <a class="reference internal" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-class docutils literal"><span class="pre">FeatureHasher</span></code></a> 的实例使用了一个哈希函数来直接确定特征在样本矩阵中的列号。
这样在可检查性上增加了速度减少了内存开销。这个类不会记住输入特征的形状，也没有 <code class="docutils literal"><span class="pre">inverse_transform</span></code> 方法。</p>
<p>因为哈希函数会造成不相关特征间的冲突，所以这里使用了带有签名的哈希函数。哈希值的签名决定了输出矩阵中特征的签名。
在这种情况下，哈希冲突可能会消失，不会出现错误。且所有输出矩阵的期望都是0。</p>
<p>如果传递 <code class="docutils literal"><span class="pre">non_negative=True</span></code> 参数给构造器，那么将使用绝对值。这将减少一些对冲突的控制，但是允许输出作为参数传递给估计器如:
<a class="reference internal" href="generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB" title="sklearn.naive_bayes.MultinomialNB"><code class="xref py py-class docutils literal"><span class="pre">sklearn.naive_bayes.MultinomialNB</span></code></a> 或
<a class="reference internal" href="generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2" title="sklearn.feature_selection.chi2"><code class="xref py py-class docutils literal"><span class="pre">sklearn.feature_selection.chi2</span></code></a>
特征选择器要求非负的输入。</p>
<p>类 <a class="reference internal" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-class docutils literal"><span class="pre">FeatureHasher</span></code></a> 接受mapping(如python的字典和其在 <code class="docutils literal"><span class="pre">collections</span></code> 模块中的变体)，
使用键值对 <code class="docutils literal"><span class="pre">(feature,</span> <span class="pre">value)</span></code> ，或是使用字符串string，取决于构造器参数  <code class="docutils literal"><span class="pre">input_type</span></code> 。
Mapping 被看成键值对的列表，其中单个字符串有一个隐式的值: 1 ， 所以 <code class="docutils literal"><span class="pre">['feat1',</span> <span class="pre">'feat2',</span> <span class="pre">'feat3']</span></code> 被转化为 <code class="docutils literal"><span class="pre">[('feat1',</span> <span class="pre">1),</span> <span class="pre">('feat2',</span> <span class="pre">1),</span> <span class="pre">('feat3',</span> <span class="pre">1)]</span></code>  。</p>
<p>如果一个单独特征在一个样本中出现了多次，与之相关的次数将被加和(所以 <code class="docutils literal"><span class="pre">('feat',</span> <span class="pre">2)</span></code> and <code class="docutils literal"><span class="pre">('feat',</span> <span class="pre">3.5)</span></code> 转化成 <code class="docutils literal"><span class="pre">('feat',</span> <span class="pre">5.5)</span></code> )。
类 <a class="reference internal" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-class docutils literal"><span class="pre">FeatureHasher</span></code></a> 的输出通常是一个CSR格式的 <code class="docutils literal"><span class="pre">scipy.sparse</span></code> 稀疏矩阵。</p>
<p>特征哈希可以在文本分类中使用，
但是，与 <a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal"><span class="pre">text.CountVectorizer</span></code></a> 不同,
为使分类器/哈希联合使用，请参考下方的 <a class="reference internal" href="#hashing-vectorizer"><span class="std std-ref">Vectorizing a large text corpus with the hashing trick</span></a></p>
<p>举个例子，假设有一个词级别的自然语言处理任务，需要在 <code class="docutils literal"><span class="pre">(token,</span> <span class="pre">part_of_speech)</span></code> 键值对中
提取特征，你可以使用Python的生成器函数来提取:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">token_features</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">part_of_speech</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span>
        <span class="k">yield</span> <span class="s2">&quot;numeric&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">yield</span> <span class="s2">&quot;token=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
        <span class="k">yield</span> <span class="s2">&quot;token,pos=</span><span class="si">{}</span><span class="s2">,</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">part_of_speech</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">token</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">isupper</span><span class="p">():</span>
        <span class="k">yield</span> <span class="s2">&quot;uppercase_initial&quot;</span>
    <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">isupper</span><span class="p">():</span>
        <span class="k">yield</span> <span class="s2">&quot;all_uppercase&quot;</span>
    <span class="k">yield</span> <span class="s2">&quot;pos=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">part_of_speech</span><span class="p">)</span>
</pre></div>
</div>
<p>之后 <code class="docutils literal"><span class="pre">raw_X</span></code> 为了可以传入 <code class="docutils literal"><span class="pre">FeatureHasher.transform</span></code>
可以通过如下方式构建:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">raw_X</span> <span class="o">=</span> <span class="p">(</span><span class="n">token_features</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="n">pos_tagger</span><span class="p">(</span><span class="n">tok</span><span class="p">))</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">)</span>
</pre></div>
</div>
<p>然后传入哈希:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">hasher</span> <span class="o">=</span> <span class="n">FeatureHasher</span><span class="p">(</span><span class="n">input_type</span><span class="o">=</span><span class="s1">&#39;string&#39;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">hasher</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">raw_X</span><span class="p">)</span>
</pre></div>
</div>
<p>得到一个 <code class="docutils literal"><span class="pre">scipy.sparse</span></code> 类型的的矩阵 <code class="docutils literal"><span class="pre">X</span></code> 。</p>
<p>注意对使用生成器的理解Note the use of a generator comprehension,
它将为特征哈希引入懒加载机制:
词令牌(token)只在哈希要求时处理。</p>
<div class="section" id="id5">
<h3>4.2.2.1. 实现细节<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>类 <a class="reference internal" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-class docutils literal"><span class="pre">FeatureHasher</span></code></a> 使用了有符号的MurmurHash3的变体，
因此导致 (同时由于 <code class="docutils literal"><span class="pre">scipy.sparse</span></code> 的限制),
现在支持的最大特征数量为 <img class="math" src="../_images/math/4b51351b335e6b2e1d241dae81db59a7d70b6dbb.png" alt="2^{31} - 1"/> 。</p>
<p>特征哈希的原始形式源于 Weinberger et al。
使用了两个独立的哈希函数 <img class="math" src="../_images/math/293fb39e1b93282c804a86186e721b32f829f1b2.png" alt="h"/> 和 <img class="math" src="../_images/math/c6e4ff26b53b5d382fc8d80b602a9c95be59adc4.png" alt="\xi"/>
来分别决定列下标和特征签名。
现有的实现是基于假设：MurmurHash3的符号位与其他位独立。</p>
<p>因为从哈希函数到列标只使用了简单的取模操作，因此建议使用二次方作为 <code class="docutils literal"><span class="pre">n_features</span></code> 的参数，
否则特征不会平均的分布到列中。</p>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and
Josh Attenberg (2009). <a class="reference external" href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf">Feature hashing for large scale multitask learning</a>. Proc. ICML.</li>
<li><a class="reference external" href="http://code.google.com/p/smhasher/wiki/MurmurHash3">MurmurHash3</a>.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="text-feature-extraction">
<span id="id6"></span><h2>4.2.3. 文本特征提取<a class="headerlink" href="#text-feature-extraction" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id7">
<h3>4.2.3.1. 体现：词袋模型<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>文本分析是机器学习算法的主要应用领域。然而，符号文字序列不能直接传递给这些算法，
因为他们要求数值的固定长度的矩阵特征而不是可变长度的文本文档。</p>
<p>为了解决这个问题，scikit-learn为数值特征提取最常见的方式提供了一系列工具，它们是:</p>
<ul class="simple">
<li><strong>tokenizing</strong> 对每个可能的词令牌分成字符串并赋予整形的id，比如使用空格和作为令牌分割依据。</li>
<li><strong>counting</strong> 每个词令牌在文档中的出现次数。</li>
<li><strong>normalizing</strong> 在大多数的文档 / 样本中，可以减少重要的次令牌的权重。</li>
</ul>
<p>在这个体系中，特征和样本有如下定义:</p>
<ul class="simple">
<li>每个 <strong>独立令牌出现频率</strong> (归一化或未归一化)
被当做一个 <strong>(特征)feature</strong> 。</li>
<li><strong>document(文本)</strong> 中所有的令牌频率向量被看做一个多元 <strong>sample(样本)</strong> 。</li>
</ul>
<p>因此文本的集合可被表示为矩阵形式，每行一条文本，每列对应每个文本中出现的词令牌(如单个词)。</p>
<p>我们称 <strong>vectorization(向量化)</strong> 是转化文本集合为数值向量的普遍方法。这种特殊思想，包括令牌化，统计频数和归一化，被称为 <strong>Bag of Words(词袋子)</strong> 或 &#8220;Bag of n-grams&#8221; 模型。文本被词出现频率描述，完全忽略词的相对位置信息。</p>
</div>
<div class="section" id="id8">
<h3>4.2.3.2. 稀疏<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>因为大多数文本通常只使用文本词向量全集中的一个小子集，结果矩阵将有许多特征的值为0(经常超过99%)。</p>
<p>例如，一个10000个短文本集的例子(如Emails)将使用总共大约100000个不同的词，而每个文本(Email)将使用100到1000个单词。</p>
<p>为了可以在内存中储存这种矩阵，同时加速线性代数的矩阵 / 向量运算，所以通常以稀疏形式实现，例如可参考在包 <code class="docutils literal"><span class="pre">scipy.sparse</span></code> 中的实现。</p>
</div>
<div class="section" id="common-vectorizer-usage">
<h3>4.2.3.3. 通常向量化使用Common Vectorizer usage<a class="headerlink" href="#common-vectorizer-usage" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal"><span class="pre">CountVectorizer</span></code></a> 在单个类中实现了令牌化和出现频数统计:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="k">import</span> <span class="n">CountVectorizer</span>
</pre></div>
</div>
<p>这个模型有很多参数，然而初始值非常合理(请参考 <a class="reference internal" href="classes.html#text-feature-extraction-ref"><span class="std std-ref">reference documentation</span></a> 获取更多细节):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span>                     
<span class="go">CountVectorizer(analyzer=...&#39;word&#39;, binary=False, decode_error=...&#39;strict&#39;,</span>
<span class="go">        dtype=&lt;... &#39;numpy.int64&#39;&gt;, encoding=...&#39;utf-8&#39;, input=...&#39;content&#39;,</span>
<span class="go">        lowercase=True, max_df=1.0, max_features=None, min_df=1,</span>
<span class="go">        ngram_range=(1, 1), preprocessor=None, stop_words=None,</span>
<span class="go">        strip_accents=None, token_pattern=...&#39;(?u)\\b\\w\\w+\\b&#39;,</span>
<span class="go">        tokenizer=None, vocabulary=None)</span>
</pre></div>
</div>
<p>让我们使用它来使简单文本全集令牌化，并统计词频:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="s1">&#39;This is the first document.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;This is the second second document.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;And the third one.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;Is this the first document?&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>                              
<span class="go">&lt;4x9 sparse matrix of type &#39;&lt;... &#39;numpy.int64&#39;&gt;&#39;</span>
<span class="go">    with 19 stored elements in Compressed Sparse ... format&gt;</span>
</pre></div>
</div>
<p>初始设定是，令牌化字符串，提取至少两个字母的词。做这一步的函数可以显式的被调用:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">analyze</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">build_analyzer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">analyze</span><span class="p">(</span><span class="s2">&quot;This is a text document to analyze.&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39;this&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;document&#39;</span><span class="p">,</span> <span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;analyze&#39;</span><span class="p">])</span>
<span class="go">True</span>
</pre></div>
</div>
<p>每个在拟合中被分析器发现的词被指派了一个独一无二的索引，在结果矩阵中表示一列。对于列的翻译可以被如下方式检索:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39;and&#39;</span><span class="p">,</span> <span class="s1">&#39;document&#39;</span><span class="p">,</span> <span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;one&#39;</span><span class="p">,</span>
<span class="gp">... </span>     <span class="s1">&#39;second&#39;</span><span class="p">,</span> <span class="s1">&#39;the&#39;</span><span class="p">,</span> <span class="s1">&#39;third&#39;</span><span class="p">,</span> <span class="s1">&#39;this&#39;</span><span class="p">])</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>           
<span class="go">array([[0, 1, 1, 1, 0, 0, 1, 0, 1],</span>
<span class="go">       [0, 1, 0, 1, 0, 2, 1, 0, 1],</span>
<span class="go">       [1, 0, 0, 0, 1, 0, 1, 1, 0],</span>
<span class="go">       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)</span>
</pre></div>
</div>
<p>从列标到特征名的反转映射储存在向量化类 vectorizer 的属性 <code class="docutils literal"><span class="pre">vocabulary_</span></code> 中:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;document&#39;</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
<p>因此在训练集里未出现的的词将在将来调用transform方法时被完全忽略:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s1">&#39;Something completely new.&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="gp">... </span>                          
<span class="go">array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)</span>
</pre></div>
</div>
<p>注意在之前的集合中第一个和最后一个文本事实上是同一个词，因此被编码成相同的向量。特别是最后一个字符是询问形式时我们丢失了他的信息。
为了防止词组顺序颠倒,我们除了提取一元模型(1-Gram，即单字单词)，也可以提取二元模型(2-Gram):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bigram_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
<span class="gp">... </span>                                    <span class="n">token_pattern</span><span class="o">=</span><span class="s1">r&#39;\b\w+\b&#39;</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">analyze</span> <span class="o">=</span> <span class="n">bigram_vectorizer</span><span class="o">.</span><span class="n">build_analyzer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">analyze</span><span class="p">(</span><span class="s1">&#39;Bi-grams are cool!&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39;bi&#39;</span><span class="p">,</span> <span class="s1">&#39;grams&#39;</span><span class="p">,</span> <span class="s1">&#39;are&#39;</span><span class="p">,</span> <span class="s1">&#39;cool&#39;</span><span class="p">,</span> <span class="s1">&#39;bi grams&#39;</span><span class="p">,</span> <span class="s1">&#39;grams are&#39;</span><span class="p">,</span> <span class="s1">&#39;are cool&#39;</span><span class="p">])</span>
<span class="go">True</span>
</pre></div>
</div>
<p>矢量化提取的词因此变得很大，同时可以在定位模式时消歧义:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_2</span> <span class="o">=</span> <span class="n">bigram_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_2</span>
<span class="gp">... </span>                          
<span class="go">array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],</span>
<span class="go">       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],</span>
<span class="go">       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)</span>
</pre></div>
</div>
<p>特别的，疑问形式如 &#8220;Is this&#8221; 只在最后一个文档中显示:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">feature_index</span> <span class="o">=</span> <span class="n">bigram_vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;is this&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_2</span><span class="p">[:,</span> <span class="n">feature_index</span><span class="p">]</span>     
<span class="go">array([0, 0, 0, 1]...)</span>
</pre></div>
</div>
</div>
<div class="section" id="tfidf">
<span id="id9"></span><h3>4.2.3.4. Tf–idf算法 字词权值<a class="headerlink" href="#tfidf" title="Permalink to this headline">¶</a></h3>
<p>在一个巨大的文本集中，一些词会出现很多次(如 &#8220;the&#8221;, &#8220;a&#8221;, &#8220;is&#8221; in English)，且带有较少的有意义的信息。
如果我们直接把数量输入到分类器中则这些频繁词组会掩盖住那些我们关注但很少出现的词。</p>
<p>为了重新计算特征权重，将其转化成适合被分类器使用的浮点值，使用tf-idf转化非常普遍。</p>
<p>Tf意思是词语频率 <strong>term-frequency</strong> 而tf–idf意思是词语频率与转置文档频率( <strong>inverse document-frequency</strong> )的乘积。
它源于一个词权重的信息检索方式(作为搜索引擎结果的评级函数)，同时在文本分类和聚类中表现良好。</p>
<p>归一化过程已经实现于类 :class:<a href="#id10"><span class="problematic" id="id11">`</span></a>TfidfTransformer`中:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="k">import</span> <span class="n">TfidfTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">TfidfTransformer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span>   
<span class="go">TfidfTransformer(norm=...&#39;l2&#39;, smooth_idf=True, sublinear_tf=False,</span>
<span class="go">                 use_idf=True)</span>
</pre></div>
</div>
<p>请参考 <a class="reference internal" href="classes.html#text-feature-extraction-ref"><span class="std std-ref">reference documentation</span></a> 获取其他参数的更多细节。</p>
<p>让我们以下方的词频为例。第一个词在任何时候都100%显示，其他两个特征只占文档中少于50%的比例:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">counts</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>          <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>          <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>          <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>          <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>          <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tfidf</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tfidf</span>                         
<span class="go">&lt;6x3 sparse matrix of type &#39;&lt;... &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">    with 9 stored elements in Compressed Sparse ... format&gt;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tfidf</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>                        
<span class="go">array([[ 0.85...,  0.  ...,  0.52...],</span>
<span class="go">       [ 1.  ...,  0.  ...,  0.  ...],</span>
<span class="go">       [ 1.  ...,  0.  ...,  0.  ...],</span>
<span class="go">       [ 1.  ...,  0.  ...,  0.  ...],</span>
<span class="go">       [ 0.55...,  0.83...,  0.  ...],</span>
<span class="go">       [ 0.63...,  0.  ...,  0.77...]])</span>
</pre></div>
</div>
<p>每一行都被正则化，来适用欧几里得标准，每个特征的权重被方法 <code class="docutils literal"><span class="pre">fit</span></code> 计算，调用结果被存储在模型参数中:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">idf_</span>                       
<span class="go">array([ 1. ...,  2.25...,  1.84...])</span>
</pre></div>
</div>
<p>因为 tf–idf 在特征提取中经常被使用，所以有另一个类: <cite>TfidfVectorizer</cite> 在单个类中结合了所有类和类中的选择:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="k">import</span> <span class="n">TfidfVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="gp">... </span>                               
<span class="go">&lt;4x9 sparse matrix of type &#39;&lt;... &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">    with 19 stored elements in Compressed Sparse ... format&gt;</span>
</pre></div>
</div>
<p>虽然 tf–idf 正则化经常被使用，但是经常有一种情况是二元变量显示会提供更好的特征。可以使用类 <a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal"><span class="pre">CountVectorizer</span></code></a> 中的参数 <code class="docutils literal"><span class="pre">binary</span></code> 来达到这一目的。
特别的，一些估计器，如 <a class="reference internal" href="naive_bayes.html#bernoulli-naive-bayes"><span class="std std-ref">朴素贝叶斯 伯努利模型</span></a> 显式地使用离散的布尔值随机变量，同时，非常短的文本可能会影响tf-idf的值，而相比之下
二元表示(binary occurrence)会更加稳定。</p>
<p>通常情况下最好的提取特征的调整参数方式是使用基于网格搜索的交叉验证，例如使用管道(pipelining)传输特征提取器和分类器:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="../auto_examples/model_selection/grid_search_text_feature_extraction.html#example-model-selection-grid-search-text-feature-extraction-py"><span class="std std-ref">Sample pipeline for text feature extraction and evaluation</span></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="id12">
<h3>4.2.3.5. 文档编码 转码<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p>文本由字组成，而文件由字节组成。字节转化成字符依照一定的编码(<em>encoding</em>)方式。
为了在Python中使用文本文档，这些字节需要被解码(<em>decoded</em>)成Unicode字符集。
常见的编码方式有 ASCII, Latin-1 (Western Europe), KOI8-R (Russian)
和通用编码方式 UTF-8 与 UTF-16。或许也其他的方式。
.. note:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>一个编码也被称为“字符集”，但是这个名词是不准确的: 一些编码可以用单个字符表示。
</pre></div>
</div>
<p>scikit-learn中的文本特征提取器知道如何解码文本文件，但是只能通过告诉它在何种编码方式之下才行。</p>
<p>类 <a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal"><span class="pre">CountVectorizer</span></code></a> 有一个参数 <code class="docutils literal"><span class="pre">encoding</span></code> 来实现这一目的。
对于现代文本文档，正确的编码方式大多是UTF-8，它也是默认编码方式 (<code class="docutils literal"><span class="pre">encoding=&quot;utf-8&quot;</span></code>)。</p>
<p>如果你的加载的文本不是UTF-8编码，你将会得到一个  <code class="docutils literal"><span class="pre">UnicodeDecodeError</span></code> 。矢量化方法可以通过设定 <code class="docutils literal"><span class="pre">decode_error</span></code> 参数值为 <code class="docutils literal"><span class="pre">&quot;ignore&quot;</span></code>
或 <code class="docutils literal"><span class="pre">&quot;replace&quot;</span></code> 来不抛出这一错误。参考Python的函数 <code class="docutils literal"><span class="pre">bytes.decode</span></code> 得到更多细节(在Python命令行里输入 <code class="docutils literal"><span class="pre">help(bytes.decode)</span></code> )。</p>
<p>如果在解码时遇到了困难，可以尝试以下方法:</p>
<ul class="simple">
<li>找到文本的实际编码方式。文件的头部或是README文件可以告诉你编码，或是一些标准编码，你可以从文本的来源处推断编码方式。</li>
<li>你可以用通常方法，使用UNIX指令 <code class="docutils literal"><span class="pre">file</span></code> 找到它的编码方式。Python的 <code class="docutils literal"><span class="pre">chardet</span></code> 模块含有一个脚本 <code class="docutils literal"><span class="pre">chardetect.py</span></code> ，可以得到大概的编码方式，但是不应依赖它，因为并不总是正确。</li>
<li>你可以尝试UFT-8并忽略错误。解码字节数组，使用``bytes.decode(errors=&#8217;replace&#8217;)`` 来用一个无意义的字符替换所有解码错误，或在矢量化方法中设置 <code class="docutils literal"><span class="pre">decode_error='replace'</span></code> 。这可能会破坏特征的使用。</li>
<li>真实文本可能有不同来源，因此用了不同编码方式，或使用错误的解码，即与编码方式不对应。这在网络中获取的文本中很常见。python的包 <a class="reference external" href="http://github.com/LuminosoInsight/python-ftfy">ftfy</a> 可以自动检查出一些解码错误的类，所以可以尝试解码未知文本为 <code class="docutils literal"><span class="pre">latin-1</span></code> 之后使用 <code class="docutils literal"><span class="pre">ftfy</span></code> 来修正错误。</li>
<li>如果文本的编码混乱，那么它将很难整理分类(如20 Newsgroups dataset的例子)。你可以把他们退回到简单的字节编码方式，如 <code class="docutils literal"><span class="pre">latin-1</span></code> 。一些文本会显示错误，但是至少相同的字节序列意味着相同的特征。</li>
</ul>
<p>例如，下面的代码片段使用 <code class="docutils literal"><span class="pre">chardet</span></code> (没有加入scikit-learn中，需要另外安装)来计算出编码方式。
之后它把文本矢量化并打印学习的单词(特征)。输出在下方给出。</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">chardet</span>    
<span class="gp">&gt;&gt;&gt; </span><span class="n">text1</span> <span class="o">=</span> <span class="n">b</span><span class="s2">&quot;Sei mir gegr</span><span class="se">\xc3\xbc\xc3\x9f</span><span class="s2">t mein Sauerkraut&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text2</span> <span class="o">=</span> <span class="n">b</span><span class="s2">&quot;holdselig sind deine Ger</span><span class="se">\xfc</span><span class="s2">che&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text3</span> <span class="o">=</span> <span class="n">b</span><span class="s2">&quot;</span><span class="se">\xff\xfe</span><span class="s2">A</span><span class="se">\x00</span><span class="s2">u</span><span class="se">\x00</span><span class="s2">f</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">F</span><span class="se">\x00</span><span class="s2">l</span><span class="se">\x00\xfc\x00</span><span class="s2">g</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">l</span><span class="se">\x00</span><span class="s2">n</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">d</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">s</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">G</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">s</span><span class="se">\x00</span><span class="s2">a</span><span class="se">\x00</span><span class="s2">n</span><span class="se">\x00</span><span class="s2">g</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">s</span><span class="se">\x00</span><span class="s2">,</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">H</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">r</span><span class="se">\x00</span><span class="s2">z</span><span class="se">\x00</span><span class="s2">l</span><span class="se">\x00</span><span class="s2">i</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">b</span><span class="se">\x00</span><span class="s2">c</span><span class="se">\x00</span><span class="s2">h</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">n</span><span class="se">\x00</span><span class="s2">,</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">t</span><span class="se">\x00</span><span class="s2">r</span><span class="se">\x00</span><span class="s2">a</span><span class="se">\x00</span><span class="s2">g</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">i</span><span class="se">\x00</span><span class="s2">c</span><span class="se">\x00</span><span class="s2">h</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">d</span><span class="se">\x00</span><span class="s2">i</span><span class="se">\x00</span><span class="s2">c</span><span class="se">\x00</span><span class="s2">h</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">f</span><span class="se">\x00</span><span class="s2">o</span><span class="se">\x00</span><span class="s2">r</span><span class="se">\x00</span><span class="s2">t</span><span class="se">\x00</span><span class="s2">&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoded</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">chardet</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="s1">&#39;encoding&#39;</span><span class="p">])</span>
<span class="gp">... </span>           <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">,</span> <span class="n">text3</span><span class="p">)]</span>        
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">decoded</span><span class="p">)</span><span class="o">.</span><span class="n">vocabulary_</span>    
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">v</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>                           
</pre></div>
</div>
<p>(取决于 <code class="docutils literal"><span class="pre">chardet</span></code> 的版本，或许会返回第一个值错误的结果。)</p>
<p>更详细的介绍Unicode和字符编码，参考 Joel Spolsky 的 <a class="reference external" href="http://www.joelonsoftware.com/articles/Unicode.html">Absolute Minimum Every Software Developer Must Know
About Unicode</a>.</p>
</div>
<div class="section" id="id13">
<h3>4.2.3.6. 应用与例子<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>词袋子模型表示法非常简单但在实际中很有用。</p>
<p>特别的，在监督学习设置( <strong>supervised setting</strong> )中它能够把快速和可伸缩的线性模型相结合，来训练分类器( <strong>document classifiers</strong> )，例如:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="../auto_examples/text/document_classification_20newsgroups.html#example-text-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a></li>
</ul>
</div></blockquote>
<p>在 <strong>unsupervised setting</strong> 中它可以为相似文档分类，同时应用聚类方法，比如 <a class="reference internal" href="clustering.html#k-means"><span class="std std-ref">K-means</span></a> :</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="../auto_examples/text/document_clustering.html#example-text-document-clustering-py"><span class="std std-ref">Clustering text documents using k-means</span></a></li>
</ul>
</div></blockquote>
<p>最后，通过松弛聚类的约束条件(relaxing the hard assignment constraint of clustering)，发现文集中的主题是可能的，如使用 <a class="reference internal" href="decomposition.html#nmf"><span class="std std-ref">Non-negative matrix factorization (NMF or NNMF)</span></a>:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="../auto_examples/applications/topics_extraction_with_nmf_lda.html#example-applications-topics-extraction-with-nmf-lda-py"><span class="std std-ref">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</span></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="id14">
<h3>4.2.3.7. 词袋子模型表示法的限制<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p>A collection of unigrams (what bag of words is) cannot capture phrases
and multi-word expressions, effectively disregarding any word order
dependence. Additionally, the bag of words model doesn&#8217;t account for potential
misspellings or word derivations.</p>
<p>N-grams to the rescue! Instead of building a simple collection of
unigrams (n=1), one might prefer a collection of bigrams (n=2), where
occurrences of pairs of consecutive words are counted.</p>
<p>One might alternatively consider a collection of character n-grams, a
representation resilient against misspellings and derivations.</p>
<p>For example, let&#8217;s say we&#8217;re dealing with a corpus of two documents:
<code class="docutils literal"><span class="pre">['words',</span> <span class="pre">'wprds']</span></code>. The second document contains a misspelling
of the word &#8216;words&#8217;.
A simple bag of words representation would consider these two as
very distinct documents, differing in both of the two possible features.
A character 2-gram representation, however, would find the documents
matching in 4 out of 8 features, which may help the preferred classifier
decide better:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="s1">&#39;char_wb&#39;</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">counts</span> <span class="o">=</span> <span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="s1">&#39;words&#39;</span><span class="p">,</span> <span class="s1">&#39;wprds&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39; w&#39;</span><span class="p">,</span> <span class="s1">&#39;ds&#39;</span><span class="p">,</span> <span class="s1">&#39;or&#39;</span><span class="p">,</span> <span class="s1">&#39;pr&#39;</span><span class="p">,</span> <span class="s1">&#39;rd&#39;</span><span class="p">,</span> <span class="s1">&#39;s &#39;</span><span class="p">,</span> <span class="s1">&#39;wo&#39;</span><span class="p">,</span> <span class="s1">&#39;wp&#39;</span><span class="p">])</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">counts</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="go">array([[1, 1, 1, 0, 1, 1, 1, 0],</span>
<span class="go">       [1, 1, 0, 1, 1, 1, 0, 1]])</span>
</pre></div>
</div>
<p>In the above example, <code class="docutils literal"><span class="pre">'char_wb</span></code> analyzer is used, which creates n-grams
only from characters inside word boundaries (padded with space on each
side). The <code class="docutils literal"><span class="pre">'char'</span></code> analyzer, alternatively, creates n-grams that
span across words:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="s1">&#39;char_wb&#39;</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="s1">&#39;jumpy fox&#39;</span><span class="p">])</span>
<span class="gp">... </span>                               
<span class="go">&lt;1x4 sparse matrix of type &#39;&lt;... &#39;numpy.int64&#39;&gt;&#39;</span>
<span class="go">   with 4 stored elements in Compressed Sparse ... format&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39; fox &#39;</span><span class="p">,</span> <span class="s1">&#39; jump&#39;</span><span class="p">,</span> <span class="s1">&#39;jumpy&#39;</span><span class="p">,</span> <span class="s1">&#39;umpy &#39;</span><span class="p">])</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="s1">&#39;char&#39;</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="s1">&#39;jumpy fox&#39;</span><span class="p">])</span>
<span class="gp">... </span>                               
<span class="go">&lt;1x5 sparse matrix of type &#39;&lt;... &#39;numpy.int64&#39;&gt;&#39;</span>
<span class="go">    with 5 stored elements in Compressed Sparse ... format&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39;jumpy&#39;</span><span class="p">,</span> <span class="s1">&#39;mpy f&#39;</span><span class="p">,</span> <span class="s1">&#39;py fo&#39;</span><span class="p">,</span> <span class="s1">&#39;umpy &#39;</span><span class="p">,</span> <span class="s1">&#39;y fox&#39;</span><span class="p">])</span>
<span class="go">True</span>
</pre></div>
</div>
<p>The word boundaries-aware variant <code class="docutils literal"><span class="pre">char_wb</span></code> is especially interesting
for languages that use white-spaces for word separation as it generates
significantly less noisy features than the raw <code class="docutils literal"><span class="pre">char</span></code> variant in
that case. For such languages it can increase both the predictive
accuracy and convergence speed of classifiers trained using such
features while retaining the robustness with regards to misspellings and
word derivations.</p>
<p>While some local positioning information can be preserved by extracting
n-grams instead of individual words, bag of words and bag of n-grams
destroy most of the inner structure of the document and hence most of
the meaning carried by that internal structure.</p>
<p>In order to address the wider task of Natural Language Understanding,
the local structure of sentences and paragraphs should thus be taken
into account. Many such models will thus be casted as &#8220;Structured output&#8221;
problems which are currently outside of the scope of scikit-learn.</p>
</div>
<div class="section" id="vectorizing-a-large-text-corpus-with-the-hashing-trick">
<span id="hashing-vectorizer"></span><h3>4.2.3.8. Vectorizing a large text corpus with the hashing trick<a class="headerlink" href="#vectorizing-a-large-text-corpus-with-the-hashing-trick" title="Permalink to this headline">¶</a></h3>
<p>The above vectorization scheme is simple but the fact that it holds an <strong>in-
memory mapping from the string tokens to the integer feature indices</strong> (the
<code class="docutils literal"><span class="pre">vocabulary_</span></code> attribute) causes several <strong>problems when dealing with large
datasets</strong>:</p>
<ul class="simple">
<li>the larger the corpus, the larger the vocabulary will grow and hence the
memory use too,</li>
<li>fitting requires the allocation of intermediate data structures
of size proportional to that of the original dataset.</li>
<li>building the word-mapping requires a full pass over the dataset hence it is
not possible to fit text classifiers in a strictly online manner.</li>
<li>pickling and un-pickling vectorizers with a large <code class="docutils literal"><span class="pre">vocabulary_</span></code> can be very
slow (typically much slower than pickling / un-pickling flat data structures
such as a NumPy array of the same size),</li>
<li>it is not easily possible to split the vectorization work into concurrent sub
tasks as the <code class="docutils literal"><span class="pre">vocabulary_</span></code> attribute would have to be a shared state with a
fine grained synchronization barrier: the mapping from token string to
feature index is dependent on ordering of the first occurrence of each token
hence would have to be shared, potentially harming the concurrent workers&#8217;
performance to the point of making them slower than the sequential variant.</li>
</ul>
<p>It is possible to overcome those limitations by combining the &#8220;hashing trick&#8221;
(<a class="reference internal" href="#feature-hashing"><span class="std std-ref">特征哈希</span></a>) implemented by the
<a class="reference internal" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-class docutils literal"><span class="pre">sklearn.feature_extraction.FeatureHasher</span></code></a> class and the text
preprocessing and tokenization features of the <a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal"><span class="pre">CountVectorizer</span></code></a>.</p>
<p>This combination is implementing in <a class="reference internal" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-class docutils literal"><span class="pre">HashingVectorizer</span></code></a>,
a transformer class that is mostly API compatible with <a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal"><span class="pre">CountVectorizer</span></code></a>.
<a class="reference internal" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-class docutils literal"><span class="pre">HashingVectorizer</span></code></a> is stateless,
meaning that you don&#8217;t have to call <code class="docutils literal"><span class="pre">fit</span></code> on it:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="k">import</span> <span class="n">HashingVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hv</span> <span class="o">=</span> <span class="n">HashingVectorizer</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="gp">... </span>                               
<span class="go">&lt;4x10 sparse matrix of type &#39;&lt;... &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">    with 16 stored elements in Compressed Sparse ... format&gt;</span>
</pre></div>
</div>
<p>You can see that 16 non-zero feature tokens were extracted in the vector
output: this is less than the 19 non-zeros extracted previously by the
<a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal"><span class="pre">CountVectorizer</span></code></a> on the same toy corpus. The discrepancy comes from
hash function collisions because of the low value of the <code class="docutils literal"><span class="pre">n_features</span></code> parameter.</p>
<p>In a real world setting, the <code class="docutils literal"><span class="pre">n_features</span></code> parameter can be left to its
default value of <code class="docutils literal"><span class="pre">2</span> <span class="pre">**</span> <span class="pre">20</span></code> (roughly one million possible features). If memory
or downstream models size is an issue selecting a lower value such as <code class="docutils literal"><span class="pre">2</span> <span class="pre">**</span>
<span class="pre">18</span></code> might help without introducing too many additional collisions on typical
text classification tasks.</p>
<p>Note that the dimensionality does not affect the CPU training time of
algorithms which operate on CSR matrices (<code class="docutils literal"><span class="pre">LinearSVC(dual=True)</span></code>,
<code class="docutils literal"><span class="pre">Perceptron</span></code>, <code class="docutils literal"><span class="pre">SGDClassifier</span></code>, <code class="docutils literal"><span class="pre">PassiveAggressive</span></code>) but it does for
algorithms that work with CSC matrices (<code class="docutils literal"><span class="pre">LinearSVC(dual=False)</span></code>, <code class="docutils literal"><span class="pre">Lasso()</span></code>,
etc).</p>
<p>Let&#8217;s try again with the default setting:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hv</span> <span class="o">=</span> <span class="n">HashingVectorizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="gp">... </span>                              
<span class="go">&lt;4x1048576 sparse matrix of type &#39;&lt;... &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">    with 19 stored elements in Compressed Sparse ... format&gt;</span>
</pre></div>
</div>
<p>We no longer get the collisions, but this comes at the expense of a much larger
dimensionality of the output space.
Of course, other terms than the 19 used here
might still collide with each other.</p>
<p>The <a class="reference internal" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-class docutils literal"><span class="pre">HashingVectorizer</span></code></a> also comes with the following limitations:</p>
<ul class="simple">
<li>it is not possible to invert the model (no <code class="docutils literal"><span class="pre">inverse_transform</span></code> method),
nor to access the original string representation of the features,
because of the one-way nature of the hash function that performs the mapping.</li>
<li>it does not provide IDF weighting as that would introduce statefulness in the
model. A <a class="reference internal" href="generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="xref py py-class docutils literal"><span class="pre">TfidfTransformer</span></code></a> can be appended to it in a pipeline if
required.</li>
</ul>
</div>
<div class="section" id="performing-out-of-core-scaling-with-hashingvectorizer">
<h3>4.2.3.9. Performing out-of-core scaling with HashingVectorizer<a class="headerlink" href="#performing-out-of-core-scaling-with-hashingvectorizer" title="Permalink to this headline">¶</a></h3>
<p>An interesting development of using a <a class="reference internal" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-class docutils literal"><span class="pre">HashingVectorizer</span></code></a> is the ability
to perform <a class="reference external" href="http://en.wikipedia.org/wiki/Out-of-core_algorithm">out-of-core</a> scaling. This means that we can learn from data that
does not fit into the computer&#8217;s main memory.</p>
<p>A strategy to implement out-of-core scaling is to stream data to the estimator
in mini-batches. Each mini-batch is vectorized using <a class="reference internal" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-class docutils literal"><span class="pre">HashingVectorizer</span></code></a>
so as to guarantee that the input space of the estimator has always the same
dimensionality. The amount of memory used at any time is thus bounded by the
size of a mini-batch. Although there is no limit to the amount of data that can
be ingested using such an approach, from a practical point of view the learning
time is often limited by the CPU time one wants to spend on the task.</p>
<p>For a full-fledged example of out-of-core scaling in a text classification
task see <a class="reference internal" href="../auto_examples/applications/plot_out_of_core_classification.html#example-applications-plot-out-of-core-classification-py"><span class="std std-ref">Out-of-core classification of text documents</span></a>.</p>
</div>
<div class="section" id="customizing-the-vectorizer-classes">
<h3>4.2.3.10. Customizing the vectorizer classes<a class="headerlink" href="#customizing-the-vectorizer-classes" title="Permalink to this headline">¶</a></h3>
<p>It is possible to customize the behavior by passing a callable
to the vectorizer constructor:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">my_tokenizer</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">my_tokenizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">build_analyzer</span><span class="p">()(</span><span class="s2">u&quot;Some... punctuation!&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39;some...&#39;</span><span class="p">,</span> <span class="s1">&#39;punctuation!&#39;</span><span class="p">])</span>
<span class="go">True</span>
</pre></div>
</div>
<p>In particular we name:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">preprocessor</span></code>: a callable that takes an entire document as input (as a
single string), and returns a possibly transformed version of the document,
still as an entire string. This can be used to remove HTML tags, lowercase
the entire document, etc.</li>
<li><code class="docutils literal"><span class="pre">tokenizer</span></code>: a callable that takes the output from the preprocessor
and splits it into tokens, then returns a list of these.</li>
<li><code class="docutils literal"><span class="pre">analyzer</span></code>: a callable that replaces the preprocessor and tokenizer.
The default analyzers all call the preprocessor and tokenizer, but custom
analyzers will skip this. N-gram extraction and stop word filtering take
place at the analyzer level, so a custom analyzer may have to reproduce
these steps.</li>
</ul>
</div></blockquote>
<p>(Lucene users might recognize these names, but be aware that scikit-learn
concepts may not map one-to-one onto Lucene concepts.)</p>
<p>To make the preprocessor, tokenizer and analyzers aware of the model
parameters it is possible to derive from the class and override the
<code class="docutils literal"><span class="pre">build_preprocessor</span></code>, <code class="docutils literal"><span class="pre">build_tokenizer</span></code> and <code class="docutils literal"><span class="pre">build_analyzer</span></code>
factory methods instead of passing custom functions.</p>
<p>Some tips and tricks:</p>
<blockquote>
<div><ul>
<li><p class="first">If documents are pre-tokenized by an external package, then store them in
files (or strings) with the tokens separated by whitespace and pass
<code class="docutils literal"><span class="pre">analyzer=str.split</span></code></p>
</li>
<li><p class="first">Fancy token-level analysis such as stemming, lemmatizing, compound
splitting, filtering based on part-of-speech, etc. are not included in the
scikit-learn codebase, but can be added by customizing either the
tokenizer or the analyzer.
Here&#8217;s a <code class="docutils literal"><span class="pre">CountVectorizer</span></code> with a tokenizer and lemmatizer using
<a class="reference external" href="http://www.nltk.org">NLTK</a>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk</span> <span class="k">import</span> <span class="n">word_tokenize</span>          
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="k">import</span> <span class="n">WordNetLemmatizer</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">LemmaTokenizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">wnl</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">doc</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">wnl</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">doc</span><span class="p">)]</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">LemmaTokenizer</span><span class="p">())</span>  
</pre></div>
</div>
<p>(Note that this will not filter out punctuation.)</p>
</li>
</ul>
</div></blockquote>
<p>Customizing the vectorizer can also be useful when handling Asian languages
that do not use an explicit word separator such as whitespace.</p>
</div>
</div>
<div class="section" id="image-feature-extraction">
<span id="id15"></span><h2>4.2.4. Image feature extraction<a class="headerlink" href="#image-feature-extraction" title="Permalink to this headline">¶</a></h2>
<div class="section" id="patch-extraction">
<h3>4.2.4.1. Patch extraction<a class="headerlink" href="#patch-extraction" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference internal" href="generated/sklearn.feature_extraction.image.extract_patches_2d.html#sklearn.feature_extraction.image.extract_patches_2d" title="sklearn.feature_extraction.image.extract_patches_2d"><code class="xref py py-func docutils literal"><span class="pre">extract_patches_2d</span></code></a> function extracts patches from an image stored
as a two-dimensional array, or three-dimensional with color information along
the third axis. For rebuilding an image from all its patches, use
<a class="reference internal" href="generated/sklearn.feature_extraction.image.reconstruct_from_patches_2d.html#sklearn.feature_extraction.image.reconstruct_from_patches_2d" title="sklearn.feature_extraction.image.reconstruct_from_patches_2d"><code class="xref py py-func docutils literal"><span class="pre">reconstruct_from_patches_2d</span></code></a>. For example let use generate a 4x4 pixel
picture with 3 color channels (e.g. in RGB format):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="k">import</span> <span class="n">image</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">one_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">one_image</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># R channel of a fake RGB picture</span>
<span class="go">array([[ 0,  3,  6,  9],</span>
<span class="go">       [12, 15, 18, 21],</span>
<span class="go">       [24, 27, 30, 33],</span>
<span class="go">       [36, 39, 42, 45]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">extract_patches_2d</span><span class="p">(</span><span class="n">one_image</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">max_patches</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(2, 2, 2, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="go">array([[[ 0,  3],</span>
<span class="go">        [12, 15]],</span>

<span class="go">       [[15, 18],</span>
<span class="go">        [27, 30]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">extract_patches_2d</span><span class="p">(</span><span class="n">one_image</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(9, 2, 2, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="go">array([[15, 18],</span>
<span class="go">       [27, 30]])</span>
</pre></div>
</div>
<p>Let us now try to reconstruct the original image from the patches by averaging
on overlapping areas:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">reconstructed</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">reconstruct_from_patches_2d</span><span class="p">(</span><span class="n">patches</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_array_equal</span><span class="p">(</span><span class="n">one_image</span><span class="p">,</span> <span class="n">reconstructed</span><span class="p">)</span>
</pre></div>
</div>
<p>The <a class="reference internal" href="generated/sklearn.feature_extraction.image.PatchExtractor.html#sklearn.feature_extraction.image.PatchExtractor" title="sklearn.feature_extraction.image.PatchExtractor"><code class="xref py py-class docutils literal"><span class="pre">PatchExtractor</span></code></a> class works in the same way as
<a class="reference internal" href="generated/sklearn.feature_extraction.image.extract_patches_2d.html#sklearn.feature_extraction.image.extract_patches_2d" title="sklearn.feature_extraction.image.extract_patches_2d"><code class="xref py py-func docutils literal"><span class="pre">extract_patches_2d</span></code></a>, only it supports multiple images as input. It is
implemented as an estimator, so it can be used in pipelines. See:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">five_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">PatchExtractor</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">five_images</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(45, 2, 2, 3)</span>
</pre></div>
</div>
</div>
<div class="section" id="connectivity-graph-of-an-image">
<h3>4.2.4.2. Connectivity graph of an image<a class="headerlink" href="#connectivity-graph-of-an-image" title="Permalink to this headline">¶</a></h3>
<p>Several estimators in the scikit-learn can use connectivity information between
features or samples. For instance Ward clustering
(<a class="reference internal" href="clustering.html#hierarchical-clustering"><span class="std std-ref">Hierarchical clustering</span></a>) can cluster together only neighboring pixels
of an image, thus forming contiguous patches:</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_lena_ward_segmentation.html"><img alt="modules/../auto_examples/cluster/images/plot_lena_ward_segmentation_001.png" src="modules/../auto_examples/cluster/images/plot_lena_ward_segmentation_001.png" /></a>
</div>
<p>For this purpose, the estimators use a &#8216;connectivity&#8217; matrix, giving
which samples are connected.</p>
<p>The function <a class="reference internal" href="generated/sklearn.feature_extraction.image.img_to_graph.html#sklearn.feature_extraction.image.img_to_graph" title="sklearn.feature_extraction.image.img_to_graph"><code class="xref py py-func docutils literal"><span class="pre">img_to_graph</span></code></a> returns such a matrix from a 2D or 3D
image. Similarly, <a class="reference internal" href="generated/sklearn.feature_extraction.image.grid_to_graph.html#sklearn.feature_extraction.image.grid_to_graph" title="sklearn.feature_extraction.image.grid_to_graph"><code class="xref py py-func docutils literal"><span class="pre">grid_to_graph</span></code></a> build a connectivity matrix for
images given the shape of these image.</p>
<p>These matrices can be used to impose connectivity in estimators that use
connectivity information, such as Ward clustering
(<a class="reference internal" href="clustering.html#hierarchical-clustering"><span class="std std-ref">Hierarchical clustering</span></a>), but also to build precomputed kernels,
or similarity matrices.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>Examples</strong></p>
<ul class="last simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_lena_ward_segmentation.html#example-cluster-plot-lena-ward-segmentation-py"><span class="std std-ref">A demo of structured Ward hierarchical clustering on Lena image</span></a></li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_segmentation_toy.html#example-cluster-plot-segmentation-toy-py"><span class="std std-ref">Spectral clustering for image segmentation</span></a></li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#example-cluster-plot-feature-agglomeration-vs-univariate-selection-py"><span class="std std-ref">Feature agglomeration vs. univariate selection</span></a></li>
</ul>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2010 - 2014, scikit-learn developers (BSD License).
      <a href="../_sources/modules/feature_extraction.txt" rel="nofollow">Show this page source</a>
    </div>
     <div class="rel">
    
    <div class="buttonPrevious">
      <a href="pipeline.html">Previous
      </a>
    </div>
    
     </div>

    
    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-22606712-2']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
    

    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript"> google.load('search', '1',
        {language : 'en'}); google.setOnLoadCallback(function() {
            var customSearchControl = new
            google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
            customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
            var options = new google.search.DrawOptions();
            options.setAutoComplete(true);
            customSearchControl.draw('cse', options); }, true);
    </script>
  </body>
</html>