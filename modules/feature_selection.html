
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
  
    <title>1.13. 特征选择(Feature selection) &#8212; scikit-learn 0.18.1 documentation</title>
  <!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" media="screen" />
  <link rel="stylesheet" href="../_static/css/bootstrap-responsive.css"/>

    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.18.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/js/copybutton.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="top" title="scikit-learn 0.18.1 documentation" href="../index.html" />
    <link rel="up" title="1. Supervised learning" href="../supervised_learning.html" />
    <link rel="next" title="1.14. Semi-Supervised" href="label_propagation.html" />
    <link rel="prev" title="1.12. Multiclass and multilabel algorithms" href="multiclass.html" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="../_static/js/bootstrap.min.js" type="text/javascript"></script>
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/feature_selection.html" />

  <script type="text/javascript">
    $("div.buttonNext, div.buttonPrevious").hover(
       function () {
           $(this).css('background-color', '#FF9C34');
       },
       function () {
           $(this).css('background-color', '#A7D6E2');
       }
    );
  </script>

  </head>
  <body role="document">

<div class="header-wrapper">
    <div class="header">
        <p class="logo"><a href="../index.html">
            <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
        </a>
        </p><div class="navbar">
            <ul>
                <li><a href="../index.html">主页</a></li>
                <li><a href="../install.html">安装</a></li>
                <li class="btn-li"><div class="btn-group">
              <a href="../documentation.html">文档</a>
              <a class="btn dropdown-toggle" data-toggle="dropdown">
                 <span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
            <li class="link-title">Scikit-learn 0.17 (stable)</li>
            <li><a href="../tutorial/index.html">入门指南</a></li>
            <li><a href="../user_guide.html">使用手册</a></li>
            <li><a href="classes.html">API</a></li>
            <li><a href="../faq.html">FAQ</a></li>
            <li><a href="../developers.html">贡献</a></li>
            <li class="divider"></li>
                <li><a href="http://scikit-learn.org/dev/documentation.html">Scikit-learn 0.18 (development)</a></li>
                <li><a href="http://scikit-learn.org/0.16/documentation.html">Scikit-learn 0.16</a></li>
				<li><a href="../_downloads/user_guide.pdf">PDF 文档</a></li>
              </ul>
            </div>
        </li>
            <li><a href="../auto_examples/index.html">例子</a></li>
            </ul>

            <div class="search_form">
                <div id="cse" style="width: 100%;"></div>
            </div>
        </div> <!-- end navbar --></div>
</div>


<!-- Github "fork me" ribbon -->
<a href="https://github.com/lzjqsdd/scikit-learn-doc-cn">
  <img class="fork-me"
       style="position: absolute; top: 0; right: 0; border: 0;"
       src="../_static/img/forkme.png"
       alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
    <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
        <div class="rel">
    

  <!-- rellinks[1:] is an ugly hack to avoid link to module
  index -->
        <div class="rellink">
        <a href="multiclass.html"
        accesskey="P">Previous
        <br/>
        <span class="smallrellink">
        1.12. Multicl...
        </span>
            <span class="hiddenrellink">
            1.12. Multiclass and multilabel algorithms
            </span>
        </a>
        </div>

    <!-- Ad a link to the 'up' page -->
        <div class="spacer">
        &nbsp;
        </div>
        <div class="rellink">
        <a href="../supervised_learning.html">
        Up
        <br/>
        <span class="smallrellink">
        1. Supervised...
        </span>
            <span class="hiddenrellink">
            1. Supervised learning
            </span>
            
        </a>
        </div>
    </div>
    
      <p class="doc-version">This documentation is for scikit-learn <strong>version 0.18.1</strong> &mdash; <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
    <p class="citing">If you use the software, please consider <a href="../about.html#citing-scikit-learn">citing scikit-learn</a>.</p>
    <ul>
<li><a class="reference internal" href="#">1.13. 特征选择(Feature selection)</a><ul>
<li><a class="reference internal" href="#removing-features-with-low-variance">1.13.1. 移除低方差的特征(Removing features with low variance)</a></li>
<li><a class="reference internal" href="#univariate-feature-selection">1.13.2. 单变量特征选择(Univariate feature selection)</a></li>
<li><a class="reference internal" href="#recursive-feature-elimination">1.13.3. 递归特征消除(Recursive feature elimination)</a></li>
<li><a class="reference internal" href="#selectfrommodel-feature-selection-using-selectfrommodel">1.13.4. 使用SelectFromModel选择特征(Feature selection using SelectFromModel)</a><ul>
<li><a class="reference internal" href="#l1-l1-based-feature-selection">1.13.4.1. 基于L1的特征选择(L1-based feature selection)</a></li>
<li><a class="reference internal" href="#randomized-sparse-models">1.13.4.2. 随机稀疏模型(Randomized sparse models)</a></li>
<li><a class="reference internal" href="#tree-based-feature-selection">1.13.4.3. 基于树的特征选择(Tree-based feature selection)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pipeline-feature-selection-as-part-of-a-pipeline">1.13.5. 特征选择融入pipeline(Feature selection as part of a pipeline)</a></li>
</ul>
</li>
</ul>

    </div>
</div>

<input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
<label for="nav-trigger"></label>




      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="feature-selection">
<span id="id1"></span><h1>1.13. 特征选择(Feature selection)<a class="headerlink" href="#feature-selection" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div><a class="reference internal" href="classes.html#module-sklearn.feature_selection" title="sklearn.feature_selection"><code class="xref py py-mod docutils literal"><span class="pre">sklearn.feature_selection</span></code></a> 模块中的类能够用于数据集的特征选择/降维，以此来提高预测模型的准确率或改善它们在高维数据集上的表现。</div></blockquote>
<div class="section" id="removing-features-with-low-variance">
<span id="variance-threshold"></span><h2>1.13.1. 移除低方差的特征(Removing features with low variance)<a class="headerlink" href="#removing-features-with-low-variance" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold" title="sklearn.feature_selection.VarianceThreshold"><code class="xref py py-class docutils literal"><span class="pre">VarianceThreshold</span></code></a> 是特征选择中的一项基本方法。它会移除所有方差不满足阈值的特征。默认设置下，它将移除所有方差为0的特征，即那些在所有样本中数值完全相同的特征。</p>
<p>假设我们有一个带有布尔特征的数据集，我们要移除那些超过80%的数据都为1或0的特征。布尔特征是伯努利随机变量，该类变量的方差为：</p>
<div class="math">
<p><img src="../_images/math/415d9594608f0ddd0c80635c2fcce650e402d81b.png" alt="\mathrm{Var}[X] = p(1 - p)"/></p>
</div><p>我们可以使用阈值 <code class="docutils literal"><span class="pre">.8</span> <span class="pre">*</span> <span class="pre">(1</span> <span class="pre">-</span> <span class="pre">.8)</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="k">import</span> <span class="n">VarianceThreshold</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="p">(</span><span class="o">.</span><span class="mi">8</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="o">.</span><span class="mi">8</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[0, 1],</span>
<span class="go">       [1, 0],</span>
<span class="go">       [0, 0],</span>
<span class="go">       [1, 1],</span>
<span class="go">       [1, 0],</span>
<span class="go">       [1, 1]])</span>
</pre></div>
</div>
<p>果然, <code class="docutils literal"><span class="pre">VarianceThreshold</span></code> 移除了第一列特征，第一列中特征值为0的概率达到了 <img class="math" src="../_images/math/e4d3834155b28b8479d7be11cc57a67375095c07.png" alt="p = 5/6 &gt; .8"/> 。</p>
</div>
<div class="section" id="univariate-feature-selection">
<span id="id2"></span><h2>1.13.2. 单变量特征选择(Univariate feature selection)<a class="headerlink" href="#univariate-feature-selection" title="Permalink to this headline">¶</a></h2>
<p>单变量特征选择基于单变量的统计测试来选择最佳特征。它可以看作预测模型的一项预处理。Scikit-learn将特征选择程序用包含 <code class="docutils literal"><span class="pre">transform</span></code> 函数的对象来展现：</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" title="sklearn.feature_selection.SelectKBest"><code class="xref py py-class docutils literal"><span class="pre">SelectKBest</span></code></a> 移除得分前 <img class="math" src="../_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"/> 名以外的所有特征</li>
<li><a class="reference internal" href="generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile" title="sklearn.feature_selection.SelectPercentile"><code class="xref py py-class docutils literal"><span class="pre">SelectPercentile</span></code></a> 移除得分在用户指定百分比以后的特征</li>
<li>对每个特征使用通用的单变量统计测试：
假正率(false positive rate) <a class="reference internal" href="generated/sklearn.feature_selection.SelectFpr.html#sklearn.feature_selection.SelectFpr" title="sklearn.feature_selection.SelectFpr"><code class="xref py py-class docutils literal"><span class="pre">SelectFpr</span></code></a>, 伪发现率(false discovery rate)
<a class="reference internal" href="generated/sklearn.feature_selection.SelectFdr.html#sklearn.feature_selection.SelectFdr" title="sklearn.feature_selection.SelectFdr"><code class="xref py py-class docutils literal"><span class="pre">SelectFdr</span></code></a>, 或族系误差率 <a class="reference internal" href="generated/sklearn.feature_selection.SelectFwe.html#sklearn.feature_selection.SelectFwe" title="sklearn.feature_selection.SelectFwe"><code class="xref py py-class docutils literal"><span class="pre">SelectFwe</span></code></a>.</li>
<li><a class="reference internal" href="generated/sklearn.feature_selection.GenericUnivariateSelect.html#sklearn.feature_selection.GenericUnivariateSelect" title="sklearn.feature_selection.GenericUnivariateSelect"><code class="xref py py-class docutils literal"><span class="pre">GenericUnivariateSelect</span></code></a> 可以设置不同的策略来进行单变量特征选择。同时不同的选择策略也能够使用超参数寻优，从而让我们找到最佳的单变量特征选择策略。</li>
</ul>
</div></blockquote>
<p>比如，我们可以对样本进行一次 :math:<a href="#id3"><span class="problematic" id="id4">`</span></a>chi^2 测试来选择最佳的两项特征：</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="k">import</span> <span class="n">SelectKBest</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="k">import</span> <span class="n">chi2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(150, 4)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(150, 2)</span>
</pre></div>
</div>
<p>这些作为打分函数输入的对象，返回单变量的概率值：</p>
<blockquote>
<div><ul class="simple">
<li>用于回归: <a class="reference internal" href="generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression" title="sklearn.feature_selection.f_regression"><code class="xref py py-func docutils literal"><span class="pre">f_regression</span></code></a></li>
<li>用于分类: <a class="reference internal" href="generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2" title="sklearn.feature_selection.chi2"><code class="xref py py-func docutils literal"><span class="pre">chi2</span></code></a> or <a class="reference internal" href="generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif" title="sklearn.feature_selection.f_classif"><code class="xref py py-func docutils literal"><span class="pre">f_classif</span></code></a></li>
</ul>
</div></blockquote>
<div class="topic">
<p class="topic-title first">稀疏数据的特征选择</p>
<p>如果你使用稀疏数据 (比如，使用稀疏矩阵表示的数据),
只有 <a class="reference internal" href="generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2" title="sklearn.feature_selection.chi2"><code class="xref py py-func docutils literal"><span class="pre">chi2</span></code></a> 能在处理数据时保持其稀疏性.</p>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<p><a class="reference internal" href="../auto_examples/feature_selection/plot_feature_selection.html#example-feature-selection-plot-feature-selection-py"><span class="std std-ref">Univariate Feature Selection</span></a></p>
</div>
</div>
<div class="section" id="recursive-feature-elimination">
<span id="rfe"></span><h2>1.13.3. 递归特征消除(Recursive feature elimination)<a class="headerlink" href="#recursive-feature-elimination" title="Permalink to this headline">¶</a></h2>
<p>对于一个为数据特征指定权重的预测模型（例如，线性模型对应参数coefficients），递归特征消除 (<a class="reference internal" href="generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE" title="sklearn.feature_selection.RFE"><code class="xref py py-class docutils literal"><span class="pre">RFE</span></code></a>)通过递归减少考察的特征集规模来选择特征。首先，预测模型在原始特征上训练，每项特征指定一个权重。之后，那些拥有最小绝对值权重的特征被踢出特征集。如此往复递归，直至剩余的特征数量达到所需的特征数量。</p>
<p><a class="reference internal" href="generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV" title="sklearn.feature_selection.RFECV"><code class="xref py py-class docutils literal"><span class="pre">RFECV</span></code></a> 通过交叉验证的方式执行RFE，以此来选择最佳数量的特征。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/feature_selection/plot_rfe_digits.html#example-feature-selection-plot-rfe-digits-py"><span class="std std-ref">Recursive feature elimination</span></a>: 一个递归特征消除的示例，展示了在数字分类任务中，像素之间的相关性。</li>
<li><a class="reference internal" href="../auto_examples/feature_selection/plot_rfe_with_cross_validation.html#example-feature-selection-plot-rfe-with-cross-validation-py"><span class="std std-ref">Recursive feature elimination with cross-validation</span></a>: 一个递归特征消除示例，通过交叉验证的方式自动调整所选特征的数量。</li>
</ul>
</div>
</div>
<div class="section" id="selectfrommodel-feature-selection-using-selectfrommodel">
<span id="select-from-model"></span><h2>1.13.4. 使用SelectFromModel选择特征(Feature selection using SelectFromModel)<a class="headerlink" href="#selectfrommodel-feature-selection-using-selectfrommodel" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" title="sklearn.feature_selection.SelectFromModel"><code class="xref py py-class docutils literal"><span class="pre">SelectFromModel</span></code></a> 作为meta-transformer，能够用于拟合后任何拥有``coef_`` 或``feature_importances_`` 属性的预测模型。
如果特征对应的``coef_`` 或 <code class="docutils literal"><span class="pre">feature_importances_</span></code> 值低于设定的阈值``threshold``，那么这些特征将被移除。除了手动设置阈值，也可通过字符串参数调用内置的启发式算法(heuristics)来设置阈值，包括：平均值(&#8220;mean&#8221;), 中位数(&#8220;median&#8221;)以及他们与浮点数的乘积，如&#8221;0.1*mean&#8221;。</p>
<p>关于SelectFromModel使用的示例见下节。</p>
<div class="topic">
<p class="topic-title first">Examples</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/feature_selection/plot_select_from_model_boston.html#example-feature-selection-plot-select-from-model-boston-py"><span class="std std-ref">Feature selection using SelectFromModel and LassoCV</span></a>: 在阈值未知的前提下，选择了Boston dataset中两项最重要的特征。</li>
</ul>
</div>
<div class="section" id="l1-l1-based-feature-selection">
<span id="l1-feature-selection"></span><h3>1.13.4.1. 基于L1的特征选择(L1-based feature selection)<a class="headerlink" href="#l1-l1-based-feature-selection" title="Permalink to this headline">¶</a></h3>
<p>使用L1范数作为惩罚项的:ref:<cite>Linear models &lt;linear_model&gt;</cite> 会得到稀疏解：大部分特征对应的系数为0。当你希望减少特征的维度以用于其它分类器时，可以通过 <a class="reference internal" href="generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" title="sklearn.feature_selection.SelectFromModel"><code class="xref py py-class docutils literal"><span class="pre">feature_selection.SelectFromModel</span></code></a> 来选择不为0的系数。特别指出，常用于此目的的稀疏预测模型有 <a class="reference internal" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-class docutils literal"><span class="pre">linear_model.Lasso</span></code></a> （回归），  <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal"><span class="pre">linear_model.LogisticRegression</span></code></a> 和 <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal"><span class="pre">svm.LinearSVC</span></code></a> （分类）:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">LinearSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="k">import</span> <span class="n">SelectFromModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(150, 4)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lsvc</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">lsvc</span><span class="p">,</span> <span class="n">prefit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(150, 3)</span>
</pre></div>
</div>
<p>对于SVM和逻辑回归，参数C控制稀疏性：C越小，被选中的特征越少。对于Lasso，参数alpha越大，被选中的特征越少 。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/text/document_classification_20newsgroups.html#example-text-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a>:
不同算法使用基于L1的特征选择进行文档分类的对比。</li>
</ul>
</div>
<div class="topic" id="compressive-sensing">
<p class="topic-title first"><strong>L1恢复和压缩感知(L1-recovery and compressive sensing)</strong></p>
<p>对于一个好的alpha值，在满足特定条件下， <a class="reference internal" href="linear_model.html#lasso"><span class="std std-ref">Lasso</span></a> 仅使用少量观测值就能够完全恢复出非零的系数。特别地，样本的数量需要“足够大”，否则L1模型的表现会充满随机性，所谓“足够大”取决于非零系数的数量，特征数量的对数，噪声的数量，非零系数的最小绝对值以及设计矩阵X的结构。此外，设计矩阵必须拥有特定的属性，比如不能太过相关(correlated)。
对于非零系数的恢复，还没有一个选择alpha值的通用规则 。alpha值可以通过交叉验证来设置(<code class="xref py py-class docutils literal"><span class="pre">LassoCV</span></code> or <code class="xref py py-class docutils literal"><span class="pre">LassoLarsCV</span></code>)，尽管这也许会导致模型欠惩罚(under-penalized)：引入少量非相关变量不会影响分数预测。相反BIC (<code class="xref py py-class docutils literal"><span class="pre">LassoLarsIC</span></code>) 更倾向于设置较大的alpha值。</p>
<p><strong>Reference</strong> Richard G. Baraniuk &#8220;Compressive Sensing&#8221;, IEEE Signal
Processing Magazine [120] July 2007
<a class="reference external" href="http://dsp.rice.edu/files/cs/baraniukCSlecture07.pdf">http://dsp.rice.edu/files/cs/baraniukCSlecture07.pdf</a></p>
</div>
</div>
<div class="section" id="randomized-sparse-models">
<span id="randomized-l1"></span><h3>1.13.4.2. 随机稀疏模型(Randomized sparse models)<a class="headerlink" href="#randomized-sparse-models" title="Permalink to this headline">¶</a></h3>
<p>基于L1的稀疏模型的局限在于，当面对一组互相关的特征时，它们只会选择其中一项特征。为了减轻该问题的影响可以使用随机化技术，通过多次重新估计稀疏模型来扰乱设计矩阵，或通过多次下采样数据来统计一个给定的回归量被选中的次数。</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.RandomizedLasso.html#sklearn.linear_model.RandomizedLasso" title="sklearn.linear_model.RandomizedLasso"><code class="xref py py-class docutils literal"><span class="pre">RandomizedLasso</span></code></a> 实现了使用这项策略的Lasso， <a class="reference internal" href="generated/sklearn.linear_model.RandomizedLogisticRegression.html#sklearn.linear_model.RandomizedLogisticRegression" title="sklearn.linear_model.RandomizedLogisticRegression"><code class="xref py py-class docutils literal"><span class="pre">RandomizedLogisticRegression</span></code></a> 使用逻辑回归，适用于分类任务。要得到整个迭代过程的稳定分数，你可以使用   <a class="reference internal" href="generated/sklearn.linear_model.lasso_stability_path.html#sklearn.linear_model.lasso_stability_path" title="sklearn.linear_model.lasso_stability_path"><code class="xref py py-func docutils literal"><span class="pre">lasso_stability_path</span></code></a>。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_sparse_recovery.html"><img alt="../_images/plot_sparse_recovery_0031.png" src="../_images/plot_sparse_recovery_0031.png" style="width: 480.0px; height: 360.0px;" /></a>
</div>
<p>注意到对于非零特征的检测，要使随机稀疏模型比标准F统计量更有效， 那么模型的参考标准需要是稀疏的，换句话说，非零特征应当只占一小部分。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_sparse_recovery.html#example-linear-model-plot-sparse-recovery-py"><span class="std std-ref">Sparse recovery: feature selection for sparse linear models</span></a>: 比较了不同的特征选择方法，并讨论了它们各自适用的场合。</li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li>N. Meinshausen, P. Buhlmann, &#8220;Stability selection&#8221;,
Journal of the Royal Statistical Society, 72 (2010)
<a class="reference external" href="http://arxiv.org/pdf/0809.2932">http://arxiv.org/pdf/0809.2932</a></li>
<li>F. Bach, &#8220;Model-Consistent Sparse Estimation through the Bootstrap&#8221;
<a class="reference external" href="http://hal.inria.fr/hal-00354771/">http://hal.inria.fr/hal-00354771/</a></li>
</ul>
</div>
</div>
<div class="section" id="tree-based-feature-selection">
<h3>1.13.4.3. 基于树的特征选择(Tree-based feature selection)<a class="headerlink" href="#tree-based-feature-selection" title="Permalink to this headline">¶</a></h3>
<p>基于树的预测模型（见 <a class="reference internal" href="classes.html#module-sklearn.tree" title="sklearn.tree"><code class="xref py py-mod docutils literal"><span class="pre">sklearn.tree</span></code></a> 模块，森林见 <a class="reference internal" href="classes.html#module-sklearn.ensemble" title="sklearn.ensemble"><code class="xref py py-mod docutils literal"><span class="pre">sklearn.ensemble</span></code></a> 模块）能够用来计算特征的重要程度，因此能用来去除不相关的特征（结合 <a class="reference internal" href="generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" title="sklearn.feature_selection.SelectFromModel"><code class="xref py py-class docutils literal"><span class="pre">sklearn.feature_selection.SelectFromModel</span></code></a>
）:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">ExtraTreesClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="k">import</span> <span class="n">SelectFromModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(150, 4)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span>  
<span class="go">array([ 0.04...,  0.05...,  0.4...,  0.4...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">prefit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>               
<span class="go">(150, 2)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/ensemble/plot_forest_importances.html#example-ensemble-plot-forest-importances-py"><span class="std std-ref">Feature importances with forests of trees</span></a>: 从模拟数据中恢复有意义的特征。</li>
<li><a class="reference internal" href="../auto_examples/ensemble/plot_forest_importances_faces.html#example-ensemble-plot-forest-importances-faces-py"><span class="std std-ref">Pixel importances with a parallel forest of trees</span></a>: 用于人脸识别数据的示例。</li>
</ul>
</div>
</div>
</div>
<div class="section" id="pipeline-feature-selection-as-part-of-a-pipeline">
<h2>1.13.5. 特征选择融入pipeline(Feature selection as part of a pipeline)<a class="headerlink" href="#pipeline-feature-selection-as-part-of-a-pipeline" title="Permalink to this headline">¶</a></h2>
<dl class="docutils">
<dt>特征选择常常被当作学习之前的一项预处理。在scikit-learn中推荐使用</dt>
<dd><p class="first"><a class="reference internal" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="xref py py-class docutils literal"><span class="pre">sklearn.pipeline.Pipeline</span></code></a>:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="n">clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
  <span class="p">(</span><span class="s1">&#39;feature_selection&#39;</span><span class="p">,</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l1&quot;</span><span class="p">))),</span>
  <span class="p">(</span><span class="s1">&#39;classification&#39;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">())</span>
<span class="p">])</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<p>在此代码片段中，我们将 <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal"><span class="pre">sklearn.svm.LinearSVC</span></code></a> 和 <a class="reference internal" href="generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" title="sklearn.feature_selection.SelectFromModel"><code class="xref py py-class docutils literal"><span class="pre">sklearn.feature_selection.SelectFromModel</span></code></a> 结合来评估特征的重要性，并选择最相关的特征。之后 <a class="reference internal" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="xref py py-class docutils literal"><span class="pre">sklearn.ensemble.RandomForestClassifier</span></code></a> 模型使用转换后的输出训练，即只使用被选出的相关特征。你可以选择其它特征选择方法，或是其它提供特征重要性评估的分类器。更多详情见 <a class="reference internal" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="xref py py-class docutils literal"><span class="pre">sklearn.pipeline.Pipeline</span></code></a> 相关示例。</p>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2010 - 2014, scikit-learn developers (BSD License).
      <a href="../_sources/modules/feature_selection.txt" rel="nofollow">Show this page source</a>
    </div>
     <div class="rel">
    
    <div class="buttonPrevious">
      <a href="multiclass.html">Previous
      </a>
    </div>
    
     </div>

    
    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-22606712-2']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
    

    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript"> google.load('search', '1',
        {language : 'en'}); google.setOnLoadCallback(function() {
            var customSearchControl = new
            google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
            customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
            var options = new google.search.DrawOptions();
            options.setAutoComplete(true);
            customSearchControl.draw('cse', options); }, true);
    </script>
  </body>
</html>